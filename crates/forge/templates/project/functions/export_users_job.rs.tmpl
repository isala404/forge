use crate::schema::User;
use forge::prelude::*;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ExportUsersInput {
    pub format: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ExportUsersOutput {
    pub user_count: usize,
    pub data: String,
    pub format: String,
}

/// Background job demonstrating:
/// - Retry with exponential backoff
/// - Idempotency guarantee
/// - Progress reporting with ctx.progress()
/// - Heartbeat for long-running jobs with ctx.heartbeat()
/// - Retry detection with ctx.is_retry() and ctx.is_last_attempt()
/// - Priority levels (low, normal, high, critical, background)
/// - Worker capability targeting (general, gpu, high_cpu)
#[forge::job]
#[timeout = "10m"]
#[priority = "low"]
#[retry(max_attempts = 3, backoff = "exponential")]
#[idempotent]
// #[worker_capability = "general"]  // Target specific worker types
pub async fn export_users(ctx: &JobContext, input: ExportUsersInput) -> Result<ExportUsersOutput> {
    use std::time::Duration;

    // Log retry information with context
    if ctx.is_retry() {
        tracing::warn!(
            job_id = %ctx.job_id,
            attempt = ctx.attempt,
            max_attempts = ctx.max_attempts,
            is_last = ctx.is_last_attempt(),
            "Retrying export job"
        );

        // On last attempt, maybe try a different strategy
        if ctx.is_last_attempt() {
            tracing::error!(job_id = %ctx.job_id, "Final attempt - will not retry after this");
        }
    }

    tracing::info!(
        job_id = %ctx.job_id,
        format = %input.format,
        "Starting user export"
    );

    let _ = ctx.progress(0, "Initializing export...");
    tokio::time::sleep(Duration::from_millis(300)).await;

    // Send heartbeat to indicate job is still alive
    // Important for long-running jobs to prevent timeout
    ctx.heartbeat().await?;

    let _ = ctx.progress(20, "Fetching users...");
    let users: Vec<User> =
        sqlx::query_as::<_, User>("SELECT * FROM users ORDER BY created_at DESC")
            .fetch_all(ctx.db())
            .await?;

    // Send another heartbeat after database operation
    ctx.heartbeat().await?;

    let total = users.len();
    let _ = ctx.progress(50, format!("Found {} users, formatting...", total));
    tokio::time::sleep(Duration::from_millis(300)).await;

    // Format data based on requested format
    let data = match input.format.as_str() {
        "json" => {
            serde_json::to_string_pretty(&users).map_err(|e| ForgeError::Job(e.to_string()))?
        }
        _ => {
            let mut csv = String::from("id,email,name,role,created_at\n");
            for (i, user) in users.iter().enumerate() {
                csv.push_str(&format!(
                    "{},{},{},{:?},{}\n",
                    user.id, user.email, user.name, user.role, user.created_at
                ));

                // Update progress during large exports
                if i > 0 && i % 100 == 0 {
                    let progress = 50 + (i * 40 / total.max(1)) as u8;
                    let _ = ctx.progress(progress, format!("Formatted {}/{} users", i, total));
                    ctx.heartbeat().await?;
                }
            }
            csv
        }
    };

    let _ = ctx.progress(95, "Finalizing...");
    tokio::time::sleep(Duration::from_millis(200)).await;

    let _ = ctx.progress(100, format!("Exported {} users", total));

    tracing::info!(
        job_id = %ctx.job_id,
        user_count = total,
        format = %input.format,
        "Export completed"
    );

    Ok(ExportUsersOutput {
        user_count: total,
        data,
        format: input.format,
    })
}
